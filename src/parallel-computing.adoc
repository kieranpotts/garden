= Parallel computing

Parallel computing refers to computations that are performed simultaneously by
multiple processing units. Parallelism is achieved by distributing the
computation across multiple cores in a single machine, or by distributing the
computation across multiple machines (and therefore multiple processing
units) in a cluster or grid.

Parallelism increases the throughput and computational speed of a system by
using multiple processors.

Parallel computing has become the dominant computer architecture, in the
current era of multi-core processors. Meanwhile, computer clusters, massively
parallel computing, and grid computing are used for big data processing.

A commonly cited example of parallel computing is distribute data processing
systems like Hadoop, in which large-scale data processing is performed across
multiple clusters. Each cluster processes a portion of the data in parallel,
significantly reducing the overall processing time.

.Parallelism versus concurrency
****
Parallelism is a separate concept to link:./concurrency.adoc[concurrency]. A
parallel program uses multiple CPU cores, each performing a task independently.
A concurrent program uses a single CPU core but switches between tasks (ie.
threads) to make the most efficient use of CPU time.

Parallelism is true multi-tasking. Concurrency is multi-threading.

See also link:./concurrency.adoc[*Concurrency*].
****

Parallel programs are more difficult to design than sequential ones, because
several new classes of potential bugs are introduced, of which race conditions
are the most common. Requirements to communicate and synchronize data
between parallel processes also make program designs more complex.
